{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tej/Documents/Courses/Learning/ML_With_PyTorch_Scikit_Practice/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 7043.24 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6714.54 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 7051.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------\n",
    "#----------------------------HUGGINGFACE DATASET --------------------------------------\n",
    "#--------------------------------------------------------------------------------------\n",
    "import gzip\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from datasets import *\n",
    "dataset = load_dataset('json', split='train', data_files='./emos/data.jsonl')\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# dataset = dataset.select(range(1000))\n",
    "\n",
    "# 90% train, 10% test + validation\n",
    "train_testvalid = dataset.train_test_split(test_size=0.1)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# # gather everyone if you want to have a single DatasetDict\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "\n",
    "train_test_valid_dataset\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "    'distilbert-base-uncased'\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = train_test_valid_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "valid_dataset = tokenized_datasets[\"valid\"].shuffle(seed=42)\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format (type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "class EmOSDataModule(pl.LightningDataModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def prepare_data(self):\n",
    "        super().prepare_data()\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        super().setup(str)\n",
    "        pass\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=31)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=31)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertNetwork(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.valid_acc   = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.test_acc  = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "        # self.save_hyperparameters()\n",
    "\n",
    "        # self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n",
    "        # self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        #     model_name_or_path, config=self.config\n",
    "        # )\n",
    "        # self.metric = load_metric(\n",
    "        #     'glue', self.hparams.task_name, experiment_id=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        # )\n",
    "\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased', num_labels=6)\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids,attention_mask, labels):\n",
    "        out = self.model(input_ids,attention_mask=attention_mask, labels = labels)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.model.train()\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        \n",
    "        ### Forward pass\n",
    "        outputs = self(input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, labels)\n",
    "        self.log(\"train loss: \",loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.log(\"train acc :\", self.train_acc.compute())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "\n",
    "        outputs = self.model(input_ids,attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs['logits']\n",
    "        \n",
    "        loss = nn.functional.cross_entropy(logits, labels)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1).float()\n",
    "        self.valid_acc.update(preds, labels)\n",
    "        \n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        self.log(\"valid_acc\", self.valid_acc.compute(), prog_bar=True)\n",
    "\n",
    "        return {\"preds\": preds, \"labels\": labels}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "        # text_batch, label_batch, lengths = batch\n",
    "        # pred = self(text_batch, lengths)[:, 0]\n",
    "        # loss = nn.functional.binary_cross_entropy(pred, label_batch)\n",
    "\n",
    "        # pred = (pred >= 0.5).float()\n",
    "        # self.test_acc.update(pred, label_batch)\n",
    "        # self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        # self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n",
    "        # return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=5e-5)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                                | Params\n",
      "------------------------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy                  | 0     \n",
      "1 | valid_acc | MulticlassAccuracy                  | 0     \n",
      "2 | test_acc  | MulticlassAccuracy                  | 0     \n",
      "3 | model     | DistilBertForSequenceClassification | 67.0 M\n",
      "------------------------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.832   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tej/Documents/Courses/Learning/ML_With_PyTorch_Scikit_Practice/env/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:02<00:00,  0.77it/s, v_num=9, train loss: =1.460, valid_loss=1.410, valid_acc=0.394]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:03<00:00,  0.57it/s, v_num=9, train loss: =1.460, valid_loss=1.410, valid_acc=0.394]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "emos_dm = EmOSDataModule()\n",
    "emosClassifier = DistilBertNetwork()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=4, accelerator=\"auto\", enable_checkpointing=True)\n",
    "\n",
    "trainer.fit(model=emosClassifier, datamodule=emos_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
